{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction In a nutshell, IRIS is a collaborative platform for incident response analysts allowing to share investigations at a technical level. It's web application, so it can be either installed on a fixed-server, or on a laptop for roaming investigations where internet might not be available. It is born following the struggle to share long and complex investigations among analysts. Most of the current similar platforms are either commercial or targeting the incident handling and triage steps, not the investigation itself. The project is available at https://github.com/dfir-iris/iris-web I've read enough, I want to try it Iris comes in dockers - you only need Docker Compose and you'll be set in a few minutes. Follow the guide . Disclaimer Iris is in its early stage. It can already be used in production, but please set backups of the database and DO NOT expose the interface on the Internet. We highly recommended the use of a private dedicated and secured network. Some questions you might have What can I do with Iris ? You can : Run investigations with as many analysts as you want. Everyone has its own account Run as many investigations as you want in parallel For each investigation: Associate a customer Create a high level summary Create notes under the form of a Kandan board Create IOCs Create assets (computers, servers, accounts, firewalls, or anything you want) Associate IOCs with assets Obtain insights on assets and IOCs previously encountered in investigations Create a proper timeline referencing assets and IOCs Create an automated graph of the attack from the timeline Register evidences Upload and process of evidences through modular pipelines (eg: EVTX parsing and injection into a database or data visualiser) Set and attribute tasks to people to keep track of everything Register every steps of the investigation Generate a report based on templates, automatically filled with the elements registered in the investigation Generate a report of activity with every steps registered as well as the auto-registered entries for each analyst Search notes and IOCs across all investigations Develop your own module and pipeline to process evidences and fit your needs. These are processed through RabbitMQ. What can I not do with Iris ? This is the part where we need you. Iris aims to be a collaborative project, so any idea is welcome. You can head to the Github of the project and create an issue with the idea. What Iris is made of ? Iris is mostly coded in Python 3. There is also some HTML, CSS and javascript for the interface. Under the hood, Iris relies on : Flask for the web engine SQLAlchemy and PostgresSQL for the database RabbitMQ for the jobs processing Nginx for the reverse proxy Can I be involved ? Absolutely. You can either contact us directly or submit pull requests and ideas on the GitHub. Who's behind Iris ? The idea was born in the French CSIRT of Airbus Cybersecurity, and created by a small group of friendly incident response analysts. Iris has been used since its early stage in 2020, on more than a hundred investigations including complex cyberattacks. Why was Iris published ? We felt a gap and we wanted to fill it. We believe open source tools driven by communities can only make them better.","title":"Introduction"},{"location":"#introduction","text":"In a nutshell, IRIS is a collaborative platform for incident response analysts allowing to share investigations at a technical level. It's web application, so it can be either installed on a fixed-server, or on a laptop for roaming investigations where internet might not be available. It is born following the struggle to share long and complex investigations among analysts. Most of the current similar platforms are either commercial or targeting the incident handling and triage steps, not the investigation itself. The project is available at https://github.com/dfir-iris/iris-web I've read enough, I want to try it Iris comes in dockers - you only need Docker Compose and you'll be set in a few minutes. Follow the guide . Disclaimer Iris is in its early stage. It can already be used in production, but please set backups of the database and DO NOT expose the interface on the Internet. We highly recommended the use of a private dedicated and secured network.","title":"Introduction"},{"location":"#some-questions-you-might-have","text":"","title":"Some questions you might have"},{"location":"#what-can-i-do-with-iris","text":"You can : Run investigations with as many analysts as you want. Everyone has its own account Run as many investigations as you want in parallel For each investigation: Associate a customer Create a high level summary Create notes under the form of a Kandan board Create IOCs Create assets (computers, servers, accounts, firewalls, or anything you want) Associate IOCs with assets Obtain insights on assets and IOCs previously encountered in investigations Create a proper timeline referencing assets and IOCs Create an automated graph of the attack from the timeline Register evidences Upload and process of evidences through modular pipelines (eg: EVTX parsing and injection into a database or data visualiser) Set and attribute tasks to people to keep track of everything Register every steps of the investigation Generate a report based on templates, automatically filled with the elements registered in the investigation Generate a report of activity with every steps registered as well as the auto-registered entries for each analyst Search notes and IOCs across all investigations Develop your own module and pipeline to process evidences and fit your needs. These are processed through RabbitMQ.","title":"What can I do with Iris ?"},{"location":"#what-can-i-not-do-with-iris","text":"This is the part where we need you. Iris aims to be a collaborative project, so any idea is welcome. You can head to the Github of the project and create an issue with the idea.","title":"What can I not do with Iris ?"},{"location":"#what-iris-is-made-of","text":"Iris is mostly coded in Python 3. There is also some HTML, CSS and javascript for the interface. Under the hood, Iris relies on : Flask for the web engine SQLAlchemy and PostgresSQL for the database RabbitMQ for the jobs processing Nginx for the reverse proxy","title":"What Iris is made of ?"},{"location":"#can-i-be-involved","text":"Absolutely. You can either contact us directly or submit pull requests and ideas on the GitHub.","title":"Can I be involved ?"},{"location":"#whos-behind-iris","text":"The idea was born in the French CSIRT of Airbus Cybersecurity, and created by a small group of friendly incident response analysts. Iris has been used since its early stage in 2020, on more than a hundred investigations including complex cyberattacks.","title":"Who's behind Iris ?"},{"location":"#why-was-iris-published","text":"We felt a gap and we wanted to fill it. We believe open source tools driven by communities can only make them better.","title":"Why was Iris published ?"},{"location":"getting_started/","text":"Quick Start The most straight forward and recommended way to use IRIS is with Docker. This is presented here. 1. Pre-requisites Hardware Iris is not very resourceful and can be run on a small laptop (4 cores, 8Gb of RAM). However, for large organization and heavy usage, it will need to be greatly scaled up. We don't have benchmarks yet but keep in mind that the database can grow rapidly and modules can be resourceful depending on their purposes. The source code includes a burst test that you can adjust to recreate the load Iris might face. Docker Docker and docker compose are needed to build and run the project. Depending on the OS you will find all the information to install them on the official website of Docker . The platform is tested on Linux and MacOS (including Apple Silicon). While it should work on Windows, some path needed by the dockers to store permanent files might need to be changed in the dockerfiles. 2. Build You have found a home for Iris and installed Docker and Docker compose, it is time to build the containers. Iris is split on 5 Docker services, each with a different role. app - iris_webapp : The core, including web server, DB management, module management etc. db : A PostgresSQL database RabbitMQ : A RabbitMQ engine to handle jobs queuing and processing worker : Jobs handler relying on RabbitMQ nginx : A NGINX reverse proxy Each service can be built independently, which is useful when developing. In this QuickStart everything will be built at once. # Clone the iris-web repository git clone https://github.com/dfir-iris/iris-web.git cd iris-web # Copy the environment file cp .env.model .env # [... optionally, do some configuration as specified below ...] # Build the dockers docker-compose build You can skip this part if you just want to try or develop. If used in production, please configure the .env file at the root of the project: Nginx: you might want to specify your own certificate as specified above Database credentials: POSTGRES_PASSWORD and DB_PASS (you can also customise the usernames) IRIS secrets: SECRET_KEY and SECURITY_PASSWORD_SALT The very first time the app builds might take quite a while. After that if a service needs an update, the building process is faster. 3. Run One last command is needed to bring all dockers up. docker-compose up Iris will be available on the web interface, port 4433, HTTPS protocol. By default, an administrator account is created. The password is printed in stdout the very first time Iris is started. It won't be printed anymore after that. You can search for WARNING :: post_init :: create_safe_admin :: >>> in the logs to find the password. If you want to define an admin password at the first start, you can also create and define the environment variable IRIS_ADM_PASSWORD in the app docker instance (see the webApp Dockerfile). This has no effects once the administrator account is created.","title":"Quick Start"},{"location":"getting_started/#quick-start","text":"The most straight forward and recommended way to use IRIS is with Docker. This is presented here.","title":"Quick Start"},{"location":"getting_started/#1-pre-requisites","text":"","title":"1. Pre-requisites"},{"location":"getting_started/#hardware","text":"Iris is not very resourceful and can be run on a small laptop (4 cores, 8Gb of RAM). However, for large organization and heavy usage, it will need to be greatly scaled up. We don't have benchmarks yet but keep in mind that the database can grow rapidly and modules can be resourceful depending on their purposes. The source code includes a burst test that you can adjust to recreate the load Iris might face.","title":"Hardware"},{"location":"getting_started/#docker","text":"Docker and docker compose are needed to build and run the project. Depending on the OS you will find all the information to install them on the official website of Docker . The platform is tested on Linux and MacOS (including Apple Silicon). While it should work on Windows, some path needed by the dockers to store permanent files might need to be changed in the dockerfiles.","title":"Docker"},{"location":"getting_started/#2-build","text":"You have found a home for Iris and installed Docker and Docker compose, it is time to build the containers. Iris is split on 5 Docker services, each with a different role. app - iris_webapp : The core, including web server, DB management, module management etc. db : A PostgresSQL database RabbitMQ : A RabbitMQ engine to handle jobs queuing and processing worker : Jobs handler relying on RabbitMQ nginx : A NGINX reverse proxy Each service can be built independently, which is useful when developing. In this QuickStart everything will be built at once. # Clone the iris-web repository git clone https://github.com/dfir-iris/iris-web.git cd iris-web # Copy the environment file cp .env.model .env # [... optionally, do some configuration as specified below ...] # Build the dockers docker-compose build You can skip this part if you just want to try or develop. If used in production, please configure the .env file at the root of the project: Nginx: you might want to specify your own certificate as specified above Database credentials: POSTGRES_PASSWORD and DB_PASS (you can also customise the usernames) IRIS secrets: SECRET_KEY and SECURITY_PASSWORD_SALT The very first time the app builds might take quite a while. After that if a service needs an update, the building process is faster.","title":"2. Build"},{"location":"getting_started/#3-run","text":"One last command is needed to bring all dockers up. docker-compose up Iris will be available on the web interface, port 4433, HTTPS protocol. By default, an administrator account is created. The password is printed in stdout the very first time Iris is started. It won't be printed anymore after that. You can search for WARNING :: post_init :: create_safe_admin :: >>> in the logs to find the password. If you want to define an admin password at the first start, you can also create and define the environment variable IRIS_ADM_PASSWORD in the app docker instance (see the webApp Dockerfile). This has no effects once the administrator account is created.","title":"3. Run"},{"location":"roadmap/","text":"Roadmap The roadmap is constantly evolving with feedbacks we receive. We've thus moved it within a Github project. You can see it here .","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"The roadmap is constantly evolving with feedbacks we receive. We've thus moved it within a Github project. You can see it here .","title":"Roadmap"},{"location":"zqa/","text":"Q & A Cases Can I recover a deleted case ? No. Cases are deleted from the database and changes are committed. There is no coming back unless you have made backups of the database (which we recommend). Can I recover a deleted case object ? No. Every object such as IOCs, assets, events, notes, etc are immediately deleted from the database and changes are committed. Can I add a new asset type ? Yes. From a user with administrative rights, go to Advanced > Assets Types. Can I add a new IOC type ? Yes. Starting from v1.3.0, IOC types can be manipulated. Can I create two cases with the same name for the same customer ? Yes. Cases are identified with a unique number, so they can have the same name. Can I restrict the view of case to a set of users ? No. Please see Security. Can I change the name or customer of an existing case ? These changes are not possible for now but it might be implemented in future versions. Operations What is the password policy ? Can it be changed ? For now the password policy is hardcoded and cannot be changed. It should be 12 characters minimum and contains a capital letter and a number. Can I change my profile picture ? No, not for now. This wasn't a priority for us, it will be released in future versions. I lost the administrator password, can I recover it ? Passwords are hashed so they can't be recovered. But you can change it. If you have another admin user : Being logged as this user, simply head to the Advanced > Users section, and change the administrator password. If you don't have another admin user: You can't do the change via Iris, you need to update the DB manually. Danger ! Do not delete and recreate any users from the DB ! This will create inconsistencies in the relations and likely corrupt everything. Generate the hash of the new password with Python BCrypt import bcrypt print ( bcrypt . hashpw ( < new_password > , bcrypt . gensalt ()) Connect to the DB docker then the Postgresql database iris_db and update the password / # su postgres / # psql postgres = # \\c iris_db postgres = # UPDATE user SET password = '<hash>' WHERE \"user\".name == 'administrator'; Can I delete a user ? No. To keep consistencies in the database, users unfortunately cannot be deleted if they have done some activities. You can however disable them to prevent them appearing in the UI. Can I delete a customer ? No. To keep consistencies in the database, customers unfortunately cannot be deleted if they are linked to cases. Can I create organizations or groups ? No. It might be possible in future versions but for now it is better to spin up a new instance for restricted cases. Can I create more roles ? No. It might be possible in future versions but for now it is better to spin up a new instance for restricted cases. Can I prevent backrefs of assets and IOCs ? No. It might be possible in future versions but for now it is better to spin up a new instance for restricted cases. My report template is not generated and generates an error Please triple check typos in tags as there is no fault tolerance. Integration Can I enrich IOCs with external sources ? Starting from v1.4.0, it is now possible to easily develop module to enrich case objects. A module Iris VT is already provided to offer VirusTotal insights. Is there an API client ? Yes, you can find it on our Github . Security Can I restrict cases ? No. It might be possible in future versions but for now it is better to spin up a new instance for restricted cases. Can I expose IRIS on the Internet ? NO ! Please don't. This platform should only be accessible in a restricted environment. I found a security issue, can I have a bounty ? No - IRIS is free and open source so there is no bounty. Please report it as soon as possible so we can fix it. MISC What does IRIS stand for ? Originally Incident Response Investigation System. But it can be whatever you want really.","title":"Q & A"},{"location":"zqa/#q-a","text":"","title":"Q &amp; A"},{"location":"zqa/#cases","text":"Can I recover a deleted case ? No. Cases are deleted from the database and changes are committed. There is no coming back unless you have made backups of the database (which we recommend). Can I recover a deleted case object ? No. Every object such as IOCs, assets, events, notes, etc are immediately deleted from the database and changes are committed. Can I add a new asset type ? Yes. From a user with administrative rights, go to Advanced > Assets Types. Can I add a new IOC type ? Yes. Starting from v1.3.0, IOC types can be manipulated. Can I create two cases with the same name for the same customer ? Yes. Cases are identified with a unique number, so they can have the same name. Can I restrict the view of case to a set of users ? No. Please see Security. Can I change the name or customer of an existing case ? These changes are not possible for now but it might be implemented in future versions.","title":"Cases"},{"location":"zqa/#operations","text":"What is the password policy ? Can it be changed ? For now the password policy is hardcoded and cannot be changed. It should be 12 characters minimum and contains a capital letter and a number. Can I change my profile picture ? No, not for now. This wasn't a priority for us, it will be released in future versions. I lost the administrator password, can I recover it ? Passwords are hashed so they can't be recovered. But you can change it. If you have another admin user : Being logged as this user, simply head to the Advanced > Users section, and change the administrator password. If you don't have another admin user: You can't do the change via Iris, you need to update the DB manually. Danger ! Do not delete and recreate any users from the DB ! This will create inconsistencies in the relations and likely corrupt everything. Generate the hash of the new password with Python BCrypt import bcrypt print ( bcrypt . hashpw ( < new_password > , bcrypt . gensalt ()) Connect to the DB docker then the Postgresql database iris_db and update the password / # su postgres / # psql postgres = # \\c iris_db postgres = # UPDATE user SET password = '<hash>' WHERE \"user\".name == 'administrator'; Can I delete a user ? No. To keep consistencies in the database, users unfortunately cannot be deleted if they have done some activities. You can however disable them to prevent them appearing in the UI. Can I delete a customer ? No. To keep consistencies in the database, customers unfortunately cannot be deleted if they are linked to cases. Can I create organizations or groups ? No. It might be possible in future versions but for now it is better to spin up a new instance for restricted cases. Can I create more roles ? No. It might be possible in future versions but for now it is better to spin up a new instance for restricted cases. Can I prevent backrefs of assets and IOCs ? No. It might be possible in future versions but for now it is better to spin up a new instance for restricted cases. My report template is not generated and generates an error Please triple check typos in tags as there is no fault tolerance.","title":"Operations"},{"location":"zqa/#integration","text":"Can I enrich IOCs with external sources ? Starting from v1.4.0, it is now possible to easily develop module to enrich case objects. A module Iris VT is already provided to offer VirusTotal insights. Is there an API client ? Yes, you can find it on our Github .","title":"Integration"},{"location":"zqa/#security","text":"Can I restrict cases ? No. It might be possible in future versions but for now it is better to spin up a new instance for restricted cases. Can I expose IRIS on the Internet ? NO ! Please don't. This platform should only be accessible in a restricted environment. I found a security issue, can I have a bounty ? No - IRIS is free and open source so there is no bounty. Please report it as soon as possible so we can fix it.","title":"Security"},{"location":"zqa/#misc","text":"What does IRIS stand for ? Originally Incident Response Investigation System. But it can be whatever you want really.","title":"MISC"},{"location":"development/environment/","text":"Environment This documentation is not a How-To develop IRIS. It only proposes a setup to easily develop and test IRIS. It follows an issue raised on the Github of the project. It recommends the use of a hybrid development environment, as most of the time only the web-app needs to be changed: Pycharm or any Python IDE for the web-app Docker for db, nginx, celery and worker. The three later are even optional if you don't develop modules. Web-app For the webapp configuration, a specific .ini need to be created. Create config.priv.ini in source/app by copying the config.docker.ini present in the same directory. Change PG_SERVER = db to PG_SERVER = 127.0.0.1 or whatever IP is the Postgresql/docker running with That's the only configuration change needed for the app to run outside docker. The docker.priv.ini is already excluded in gitignore. Then Pycharm need to be setup with a dedicated environment, by adding a new configuration: Script path : source/run.py Python interpreter 3.9 Working directory: source To have pylint, right click on source in the directory tree and mark directory as > sources root . The requirements then need to be installed. Pycharm should detect the requirements.txt and propose to install the dependencies. Otherwise they can be installed with the following command (issued in the virtual environment) : pip3 install -r source\\requirements.txt Run Spin up the docker db docker-compose up db Run the Pycharm configuration you created The interface should be accessible on http://127.0.0.1:8000 (and https://127.0.0.1:4433 if you started the nginx docker) IRIS can now be developed and debugged on the fly. Tests in docker Once the code is working by running on Pycharm, we highly recommend testing it on Docker. To do so, the app docker need to be erased and rebuilt. docker-compose rm app docker-compose build app docker-compose up db app Development considerations If the development results in DB modification, please use Alembic and add a migration script so users don't loose their data when they upgrade.","title":"Environment"},{"location":"development/environment/#environment","text":"This documentation is not a How-To develop IRIS. It only proposes a setup to easily develop and test IRIS. It follows an issue raised on the Github of the project. It recommends the use of a hybrid development environment, as most of the time only the web-app needs to be changed: Pycharm or any Python IDE for the web-app Docker for db, nginx, celery and worker. The three later are even optional if you don't develop modules.","title":"Environment"},{"location":"development/environment/#web-app","text":"For the webapp configuration, a specific .ini need to be created. Create config.priv.ini in source/app by copying the config.docker.ini present in the same directory. Change PG_SERVER = db to PG_SERVER = 127.0.0.1 or whatever IP is the Postgresql/docker running with That's the only configuration change needed for the app to run outside docker. The docker.priv.ini is already excluded in gitignore. Then Pycharm need to be setup with a dedicated environment, by adding a new configuration: Script path : source/run.py Python interpreter 3.9 Working directory: source To have pylint, right click on source in the directory tree and mark directory as > sources root . The requirements then need to be installed. Pycharm should detect the requirements.txt and propose to install the dependencies. Otherwise they can be installed with the following command (issued in the virtual environment) : pip3 install -r source\\requirements.txt","title":"Web-app"},{"location":"development/environment/#run","text":"Spin up the docker db docker-compose up db Run the Pycharm configuration you created The interface should be accessible on http://127.0.0.1:8000 (and https://127.0.0.1:4433 if you started the nginx docker) IRIS can now be developed and debugged on the fly.","title":"Run"},{"location":"development/environment/#tests-in-docker","text":"Once the code is working by running on Pycharm, we highly recommend testing it on Docker. To do so, the app docker need to be erased and rebuilt. docker-compose rm app docker-compose build app docker-compose up db app Development considerations If the development results in DB modification, please use Alembic and add a migration script so users don't loose their data when they upgrade.","title":"Tests in docker"},{"location":"development/hooks/","text":"IRIS Hooks Introduced in IRIS v1.4.0 Hooks are a mean for modules to react on specific events that occurs on IRIS. By subscribing to a hook, a module is automatically notified when the associated event occurs. This offers a multitude of possibilities, from adding insight to IRIS objects, to pushing information to another platform or even changing how IRIS works. Types There are 3 types of hooks. On preload : Triggered before an object is processed and committed to database. It is triggered right after a request is received, and the data associated with the hook is usually the request content itself. In most of the cases, modules should not subscribe to these hooks. On postload : Triggered after an object is processed and committed to database. It is triggered after IRIS processed the request and the data associated with the hook is a usually a list of SqlAlchemy objects (such as IOC, Assets, etc). Manual : Triggered by manual action of a user. When a module subscribes to these hooks, it needs to provide a \"menu option name\" which will be display to users. When they click this option, the associated hook is triggered for this module only. Multiple manual hooks can be registered for one module. Danger on_preload hooks must run synchronously, i.e not queued in RabbitMQ. This effectively blocks the current user request until the module finishes the processing. We highly recommend to only use on_postload hooks for a better user experience. These hooks are transparent for users and rely on already verified and committed data. Handling on_preload hooks implies the data received is unsafe - directly coming from remote clients - and the module need to process the data as fast as possible. Subscribing and unsubscribing Two methods are provided by IrisModuleInterface to subscribe and unsubscribe from hooks. def register_to_hook ( module_id : int , iris_hook_name : str , manual_hook_name : str = None , run_asynchronously : bool = True ) def deregister_from_hook ( module_id : int , iris_hook_name : str ) The registration method expects the following arguments: module_id : The ID of the calling module. This information is given by IRIS when the register_hooks methods is called. iris_hook_name : The name of the hook to which subscribe. This must be one of the hook listed in the section below. manual_hook_name : The name of the UI menu that is provided to users if the registration concerns a manual hook. If nothing is provided, IRIS will create a name composed as follows : <module_name>::<hook_name> . This value is ignored if the signal is not manual. run_asynchronously : Set to True (default) to run the module in a RabbitMQ task upon hook triggering. If set to False, the module is called immediately, which have for effect to effectively block the current user request until the module finishes. This is the behavior to use for on_preload hooks. However , we strongly recommend the use of on_postload hooks to prevent any unwanted (see warning section above). The deregistration method expects the following arguments: module_id : The ID of the calling module. This information is given by IRIS when the register_hooks methods is called. iris_hook_name : The name of the hook to which unsubscribe. If the module is not subscribe to the specified hook the function returns without errors. Please see the modules documentation for more details on how to implement these methods. Available hooks The following hooks are natively available for subscription. Hook name Description on_preload_case_create Triggered on case creation, before commit in DB on_postload_case_create Triggered on case creation, after commit in DB on_preload_case_delete Triggered on case deletion, before commit in DB on_postload_case_delete Triggered on case deletion, after commit in DB on_preload_asset_create Triggered on asset creation, before commit in DB on_postload_asset_create Triggered on asset creation, after commit in DB on_preload_asset_update Triggered on asset update, before commit in DB on_postload_asset_update Triggered on asset update, after commit in DB on_preload_asset_delete Triggered on asset deletion, before commit in DB on_postload_asset_delete Triggered on asset deletion, after commit in DB on_manual_trigger_asset Triggered upon user action on_preload_note_create Triggered on note creation, before commit in DB on_postload_note_create Triggered on note creation, after commit in DB on_preload_note_update Triggered on note update, before commit in DB on_postload_note_update Triggered on note update, after commit in DB on_preload_note_delete Triggered on note deletion, before commit in DB on_postload_note_delete Triggered on note deletion, after commit in DB on_manual_trigger_note Triggered upon user action on_preload_ioc_create Triggered on ioc creation, before commit in DB on_postload_ioc_create Triggered on ioc creation, after commit in DB on_preload_ioc_update Triggered on ioc update, before commit in DB on_postload_ioc_update Triggered on ioc update, after commit in DB on_preload_ioc_delete Triggered on ioc deletion, before commit in DB on_postload_ioc_delete Triggered on ioc deletion, after commit in DB on_manual_trigger_ioc Triggered upon user action on_preload_event_create Triggered on event creation, before commit in DB on_postload_event_create Triggered on event creation, after commit in DB on_preload_event_update Triggered on event update, before commit in DB on_postload_event_update Triggered on event update, after commit in DB on_preload_event_delete Triggered on event deletion, before commit in DB on_postload_event_delete Triggered on event deletion, after commit in DB on_manual_trigger_event Triggered upon user action on_preload_evidence_create Triggered on evidence creation, before commit in DB on_postload_evidence_create Triggered on evidence creation, after commit in DB on_preload_evidence_update Triggered on evidence update, before commit in DB on_postload_evidence_update Triggered on evidence update, after commit in DB on_preload_evidence_delete Triggered on evidence deletion, before commit in DB on_postload_evidence_delete Triggered on evidence deletion, after commit in DB on_manual_trigger_evidence Triggered upon user action on_preload_task_create Triggered on task creation, before commit in DB on_postload_task_create Triggered on task creation, after commit in DB on_preload_task_update Triggered on task update, before commit in DB on_postload_task_update Triggered on task update, after commit in DB on_preload_task_delete Triggered on task deletion, before commit in DB on_postload_task_delete Triggered on task deletion, after commit in DB on_manual_trigger_task Triggered upon user action on_preload_global_task_create Triggered on global task creation, before commit in DB on_postload_global_task_create Triggered on global task creation, after commit in DB on_preload_global_task_update Triggered on task update, before commit in DB on_postload_global_task_update Triggered on global task update, after commit in DB on_preload_global_task_delete Triggered on task deletion, before commit in DB on_postload_global_task_delete Triggered on global task deletion, after commit in DB on_manual_trigger_global_task Triggered upon user action on_preload_report_create Triggered on report creation, before generation in DB on_postload_report_create Triggered on report creation, before download of the document on_preload_activities_report_create Triggered on activities report creation, before generation in DB on_postload_activities_report_create Triggered on activities report creation, before download of the document","title":"IRIS Hooks"},{"location":"development/hooks/#iris-hooks","text":"Introduced in IRIS v1.4.0 Hooks are a mean for modules to react on specific events that occurs on IRIS. By subscribing to a hook, a module is automatically notified when the associated event occurs. This offers a multitude of possibilities, from adding insight to IRIS objects, to pushing information to another platform or even changing how IRIS works.","title":"IRIS Hooks"},{"location":"development/hooks/#types","text":"There are 3 types of hooks. On preload : Triggered before an object is processed and committed to database. It is triggered right after a request is received, and the data associated with the hook is usually the request content itself. In most of the cases, modules should not subscribe to these hooks. On postload : Triggered after an object is processed and committed to database. It is triggered after IRIS processed the request and the data associated with the hook is a usually a list of SqlAlchemy objects (such as IOC, Assets, etc). Manual : Triggered by manual action of a user. When a module subscribes to these hooks, it needs to provide a \"menu option name\" which will be display to users. When they click this option, the associated hook is triggered for this module only. Multiple manual hooks can be registered for one module. Danger on_preload hooks must run synchronously, i.e not queued in RabbitMQ. This effectively blocks the current user request until the module finishes the processing. We highly recommend to only use on_postload hooks for a better user experience. These hooks are transparent for users and rely on already verified and committed data. Handling on_preload hooks implies the data received is unsafe - directly coming from remote clients - and the module need to process the data as fast as possible.","title":"Types"},{"location":"development/hooks/#subscribing-and-unsubscribing","text":"Two methods are provided by IrisModuleInterface to subscribe and unsubscribe from hooks. def register_to_hook ( module_id : int , iris_hook_name : str , manual_hook_name : str = None , run_asynchronously : bool = True ) def deregister_from_hook ( module_id : int , iris_hook_name : str ) The registration method expects the following arguments: module_id : The ID of the calling module. This information is given by IRIS when the register_hooks methods is called. iris_hook_name : The name of the hook to which subscribe. This must be one of the hook listed in the section below. manual_hook_name : The name of the UI menu that is provided to users if the registration concerns a manual hook. If nothing is provided, IRIS will create a name composed as follows : <module_name>::<hook_name> . This value is ignored if the signal is not manual. run_asynchronously : Set to True (default) to run the module in a RabbitMQ task upon hook triggering. If set to False, the module is called immediately, which have for effect to effectively block the current user request until the module finishes. This is the behavior to use for on_preload hooks. However , we strongly recommend the use of on_postload hooks to prevent any unwanted (see warning section above). The deregistration method expects the following arguments: module_id : The ID of the calling module. This information is given by IRIS when the register_hooks methods is called. iris_hook_name : The name of the hook to which unsubscribe. If the module is not subscribe to the specified hook the function returns without errors. Please see the modules documentation for more details on how to implement these methods.","title":"Subscribing and unsubscribing"},{"location":"development/hooks/#available-hooks","text":"The following hooks are natively available for subscription. Hook name Description on_preload_case_create Triggered on case creation, before commit in DB on_postload_case_create Triggered on case creation, after commit in DB on_preload_case_delete Triggered on case deletion, before commit in DB on_postload_case_delete Triggered on case deletion, after commit in DB on_preload_asset_create Triggered on asset creation, before commit in DB on_postload_asset_create Triggered on asset creation, after commit in DB on_preload_asset_update Triggered on asset update, before commit in DB on_postload_asset_update Triggered on asset update, after commit in DB on_preload_asset_delete Triggered on asset deletion, before commit in DB on_postload_asset_delete Triggered on asset deletion, after commit in DB on_manual_trigger_asset Triggered upon user action on_preload_note_create Triggered on note creation, before commit in DB on_postload_note_create Triggered on note creation, after commit in DB on_preload_note_update Triggered on note update, before commit in DB on_postload_note_update Triggered on note update, after commit in DB on_preload_note_delete Triggered on note deletion, before commit in DB on_postload_note_delete Triggered on note deletion, after commit in DB on_manual_trigger_note Triggered upon user action on_preload_ioc_create Triggered on ioc creation, before commit in DB on_postload_ioc_create Triggered on ioc creation, after commit in DB on_preload_ioc_update Triggered on ioc update, before commit in DB on_postload_ioc_update Triggered on ioc update, after commit in DB on_preload_ioc_delete Triggered on ioc deletion, before commit in DB on_postload_ioc_delete Triggered on ioc deletion, after commit in DB on_manual_trigger_ioc Triggered upon user action on_preload_event_create Triggered on event creation, before commit in DB on_postload_event_create Triggered on event creation, after commit in DB on_preload_event_update Triggered on event update, before commit in DB on_postload_event_update Triggered on event update, after commit in DB on_preload_event_delete Triggered on event deletion, before commit in DB on_postload_event_delete Triggered on event deletion, after commit in DB on_manual_trigger_event Triggered upon user action on_preload_evidence_create Triggered on evidence creation, before commit in DB on_postload_evidence_create Triggered on evidence creation, after commit in DB on_preload_evidence_update Triggered on evidence update, before commit in DB on_postload_evidence_update Triggered on evidence update, after commit in DB on_preload_evidence_delete Triggered on evidence deletion, before commit in DB on_postload_evidence_delete Triggered on evidence deletion, after commit in DB on_manual_trigger_evidence Triggered upon user action on_preload_task_create Triggered on task creation, before commit in DB on_postload_task_create Triggered on task creation, after commit in DB on_preload_task_update Triggered on task update, before commit in DB on_postload_task_update Triggered on task update, after commit in DB on_preload_task_delete Triggered on task deletion, before commit in DB on_postload_task_delete Triggered on task deletion, after commit in DB on_manual_trigger_task Triggered upon user action on_preload_global_task_create Triggered on global task creation, before commit in DB on_postload_global_task_create Triggered on global task creation, after commit in DB on_preload_global_task_update Triggered on task update, before commit in DB on_postload_global_task_update Triggered on global task update, after commit in DB on_preload_global_task_delete Triggered on task deletion, before commit in DB on_postload_global_task_delete Triggered on global task deletion, after commit in DB on_manual_trigger_global_task Triggered upon user action on_preload_report_create Triggered on report creation, before generation in DB on_postload_report_create Triggered on report creation, before download of the document on_preload_activities_report_create Triggered on activities report creation, before generation in DB on_postload_activities_report_create Triggered on activities report creation, before download of the document","title":"Available hooks"},{"location":"development/modules/","text":"Modules Introduction A DFIR-IRIS Module (DIM) is a Python package allowing to extend IRIS features. DIMs are not running constantly and are only called following specific actions done by users. We distinct two types of modules: Pipeline modules : Allow uploading and processing of evidences through modular pipelines (eg: EVTX parsing and injection into a database or data visualiser). These are called when a user Update case and select evidences to process. Processor modules : Allow processing of IRIS data upon predefined actions / hooks. (eg: be notified when a new IOC is created and get VT/MISP insights for it). These are either called automatically upon specific events, or if a user manually triggers them. Except for some triggers for processor modules, all tasks provided by DIMs are run asynchronously in RabbitMQ tasks, so they don't impact the UI. Both types of DIMs have the same structure, they only differs in their configurations and how they handle the data they receive. For that purpose, every DIM inherit from a common class named IrisModuleInterface - available here - which provides the basic structure and methods of a module. Hint To quickly start writing a new module, one can follow these tutorials . Overview Modules are instantiated upon actions (hooks, triggers, user actions) and this occurs each time the said actions occur. It implies the initiation of a module has to be very quick. In most of the case, the __init__ method should not even be overwritten. They can live either in the worker or the web-app, depending on their type and action they are handling. They can also live in both. This implies multiple instances of the same module can run at the same time. The graph below shows two modules of different types running in the worker and interacting with external elements. Modules don't have to handle the task creations or resources locks. This is handled by IRIS. They just need to process the data they received and return results in a predefined manner. Common structure The section below describes the common structure of modules. Directory structure setup.py # Setup configuration to build the module README.md # README iris_example_name # Name of the package __init__.py # Declaration of the package and main class IrisExampleConfig.py # Configuration of the module to help keep the main file clean IrisExampleInterface.py # Main class of the module module_helper # Sub module containing the helper functions of the module helper.py # for instance access to ext resource, manipulation of data helper2.py # etc. The init .py file Iris loads the modules dynamically. To do so, it needs to know the name of the main class of the module and relies on __init__.py to find this information. __iris_module_interface = \"IrisEXAMPLEInterface\" Where IrisEXAMPLEInterface is the main class of the module and inherit of the base class IrisModuleInterface . Caution Failing to provide the main class in __init__.py or having the main class inherit from IrisModuleInterface will make IRIS fail each time it attempts to load the module. The module configuration Iris needs to know what the module is doing and what services it is providing. This is done via the attributes of the main class (let's say IrisEXAMPLEInterface ). The attributes are : _module_name : string - \"human\" name presented to users. _interface_version : float - version of IrisModuleInterface used. If the version is not supported, the server will refuse to register the module. _module_version : float - version of the module itself to help users keep tracks of evolutions. _module_type : string - Type of module. The available modules types are listed in IrisModuleInterface.IrisModuleTypes _pipeline_support : bool - should be set to True if it implements a pipeline process (aka module of type pipeline_module ). _pipeline_info : dict - contains the configuration of the pipeline. The following structure must be followed: pipeline_info = { # Name of the pipeline used for internal tracking. This # must be unique among all modules so pick something really unique \"pipeline_internal_name\" : \"example_pipeline\" , # The name of the pipeline presented to the user. Use something # that will help the users to identify the right pipeline \"pipeline_human_name\" : \"Example Pipeline\" , # Arguments presented to the users when they select the pipeline \"pipeline_args\" : [ [ 'some_index' , 'required' ], [ 'example_argument' , 'optional' ] ] } _module_configuration : A list of dict. The list contains each field needed by the module. This list is shown in the Iris webpage of the module configuration. Each field in an entry is mandatory. _module_configuration = [ { \"param_name\" : \"vt_api_key\" , \"param_human_name\" : \"VT API Key\" , \"param_description\" : \"Virus total API key\" , \"default\" : None , \"mandatory\" : True , \"type\" : \"sensitive_string\" }, { \"param_name\" : \"vt_key_is_premium\" , \"param_human_name\" : \"VT Key is premium\" , \"param_description\" : \"Set to True if the VT key is premium\" , \"default\" : False , \"mandatory\" : True , \"type\" : \"bool\" }, { \"param_name\" : \"vt_ip_assign_asn_as_tag\" , \"param_human_name\" : \"Assign ASN tag to IP\" , \"param_description\" : \"Assign a new tag to IOC IPs with the ASN fetched from VT\" , \"default\" : True , \"mandatory\" : True , \"type\" : \"bool\" } ] The above example results in the following.","title":"Modules"},{"location":"development/modules/#modules","text":"","title":"Modules"},{"location":"development/modules/#introduction","text":"A DFIR-IRIS Module (DIM) is a Python package allowing to extend IRIS features. DIMs are not running constantly and are only called following specific actions done by users. We distinct two types of modules: Pipeline modules : Allow uploading and processing of evidences through modular pipelines (eg: EVTX parsing and injection into a database or data visualiser). These are called when a user Update case and select evidences to process. Processor modules : Allow processing of IRIS data upon predefined actions / hooks. (eg: be notified when a new IOC is created and get VT/MISP insights for it). These are either called automatically upon specific events, or if a user manually triggers them. Except for some triggers for processor modules, all tasks provided by DIMs are run asynchronously in RabbitMQ tasks, so they don't impact the UI. Both types of DIMs have the same structure, they only differs in their configurations and how they handle the data they receive. For that purpose, every DIM inherit from a common class named IrisModuleInterface - available here - which provides the basic structure and methods of a module. Hint To quickly start writing a new module, one can follow these tutorials .","title":"Introduction"},{"location":"development/modules/#overview","text":"Modules are instantiated upon actions (hooks, triggers, user actions) and this occurs each time the said actions occur. It implies the initiation of a module has to be very quick. In most of the case, the __init__ method should not even be overwritten. They can live either in the worker or the web-app, depending on their type and action they are handling. They can also live in both. This implies multiple instances of the same module can run at the same time. The graph below shows two modules of different types running in the worker and interacting with external elements. Modules don't have to handle the task creations or resources locks. This is handled by IRIS. They just need to process the data they received and return results in a predefined manner.","title":"Overview"},{"location":"development/modules/#common-structure","text":"The section below describes the common structure of modules.","title":"Common structure"},{"location":"development/modules/#directory-structure","text":"setup.py # Setup configuration to build the module README.md # README iris_example_name # Name of the package __init__.py # Declaration of the package and main class IrisExampleConfig.py # Configuration of the module to help keep the main file clean IrisExampleInterface.py # Main class of the module module_helper # Sub module containing the helper functions of the module helper.py # for instance access to ext resource, manipulation of data helper2.py # etc.","title":"Directory structure"},{"location":"development/modules/#the-initpy-file","text":"Iris loads the modules dynamically. To do so, it needs to know the name of the main class of the module and relies on __init__.py to find this information. __iris_module_interface = \"IrisEXAMPLEInterface\" Where IrisEXAMPLEInterface is the main class of the module and inherit of the base class IrisModuleInterface . Caution Failing to provide the main class in __init__.py or having the main class inherit from IrisModuleInterface will make IRIS fail each time it attempts to load the module.","title":"The init.py file"},{"location":"development/modules/#the-module-configuration","text":"Iris needs to know what the module is doing and what services it is providing. This is done via the attributes of the main class (let's say IrisEXAMPLEInterface ). The attributes are : _module_name : string - \"human\" name presented to users. _interface_version : float - version of IrisModuleInterface used. If the version is not supported, the server will refuse to register the module. _module_version : float - version of the module itself to help users keep tracks of evolutions. _module_type : string - Type of module. The available modules types are listed in IrisModuleInterface.IrisModuleTypes _pipeline_support : bool - should be set to True if it implements a pipeline process (aka module of type pipeline_module ). _pipeline_info : dict - contains the configuration of the pipeline. The following structure must be followed: pipeline_info = { # Name of the pipeline used for internal tracking. This # must be unique among all modules so pick something really unique \"pipeline_internal_name\" : \"example_pipeline\" , # The name of the pipeline presented to the user. Use something # that will help the users to identify the right pipeline \"pipeline_human_name\" : \"Example Pipeline\" , # Arguments presented to the users when they select the pipeline \"pipeline_args\" : [ [ 'some_index' , 'required' ], [ 'example_argument' , 'optional' ] ] } _module_configuration : A list of dict. The list contains each field needed by the module. This list is shown in the Iris webpage of the module configuration. Each field in an entry is mandatory. _module_configuration = [ { \"param_name\" : \"vt_api_key\" , \"param_human_name\" : \"VT API Key\" , \"param_description\" : \"Virus total API key\" , \"default\" : None , \"mandatory\" : True , \"type\" : \"sensitive_string\" }, { \"param_name\" : \"vt_key_is_premium\" , \"param_human_name\" : \"VT Key is premium\" , \"param_description\" : \"Set to True if the VT key is premium\" , \"default\" : False , \"mandatory\" : True , \"type\" : \"bool\" }, { \"param_name\" : \"vt_ip_assign_asn_as_tag\" , \"param_human_name\" : \"Assign ASN tag to IP\" , \"param_description\" : \"Assign a new tag to IOC IPs with the ASN fetched from VT\" , \"default\" : True , \"mandatory\" : True , \"type\" : \"bool\" } ] The above example results in the following.","title":"The module configuration"},{"location":"development/quick_start/processor/","text":"Processor modules In this tutorial, we demonstrate the steps to write a basic processor module which subscribes to a hook, and log what it receives when the hook is triggered. We will also add a configuration setting to offer our users the ability disable this feature. We'll call it IrisDummyModule . Project structure overview As described in the development module main page , the module should have the following structure. setup . py # Setup configuration to build the module README . md # README iris_dummy_module # Name of the package __init__ . py # Declaration of the package and main class IrisDummyConfig . py # Configuration of the module to help keep the main file clean IrisDummyInterface . py # Main class of the module While the module could have only one main file IrisDummyInterface.py , we recommend splitting its configuration into a new configuration file (here IrisDummyConfig.py ) to keep the code clear. There is no mandatory naming convention for the files or the class or the methods. We chose this one to keep things clear, and we recommend following the same. But it's up to you really. We will walk over these files one by one during this tutorial. Creating the interface The interface is the code that talks with IRIS. It implements methods that call and are called by the server. It needs to inherit IrisModuleInterface class from the IrisModuleInterface package . This module handles most of the methods needed by IRIS to recognize, set up and call the module. By inheriting this class in our interface, we avoid writing that part ourselves. Let's write our basic interface class. The name of the file has to be the name of the main class, that's the only constraint. We'll see later on why. iris_dummy_module/IrisDummyInterface.py #!/usr/bin/env python3 # Import the IrisInterface class from iris_interface.IrisModuleInterface import IrisModuleInterface # Create our module class class IrisDummyModule ( IrisModuleInterface ): pass That's it ! Actually this class is not doing anything right now. We'll need to add a few methods to register our hook later. But first we need to indicate to IRIS what is our main interface class. Remember, there is no convention restriction, so IRIS has no way to know which class it should instantiate to call our module. To do so, we need to set a specific variable in our __init__.py . iris_dummy_module/__init__.py # Set the __iris_module_interface variable to the name of our main class. # When IRIS instantiate the python module, it looks for \"module.__iris_module_interface\" # And then tries to instantiate the class \"__iris_module_interface.__iris_module_interface\", here 'IrisDummyModule.IrisDummyModule'. # That's why the python file must have the same name as the class. __iris_module_interface = \"IrisDummyModule\" Our module is now recognizable by IRIS Pretty simple right ? Writing the configuration The next step is to describe what the module is doing, its name, it's configuration, etc. This is done by overwriting predefines variables of the IrisModuleInterface class. Let's create our Python configuration file and go through each variables. iris_dummy_module/IrisDummyConfig.py # Import the module types list, so we can indicate the type of our module from iris_interface.IrisModuleInterface import IrisModuleTypes # Human name displayed in the GUI Manage > Modules. This can be anything, # but try to put something meaningful, so users recognize your module. module_name = \"IrisDummy\" # Description displayed when editing the module configuration in the UI. # This can be anything, module_description = \"Provides a dummy module that replies to one hook\" # Set the interface version used. This needs to be the version of # the IrisModuleInterface package. This version is check by the server to # to ensure our module can run on this specific server interface_version = 1.1 # The version of the module itself, it can be anything module_version = 1.0 # The type of the module, here processor module_type = IrisModuleTypes . module_processor # Our module is a processor type, so it doesn't offer any pipeline pipeline_support = False # Provide no pipeline information as our module don't implement any pipeline_info = {} # The configuration of the module that will be displayed and configurable # by administrators on the UI. This describes every parameter that can # be set. module_configuration = [ { \"param_name\" : \"log_received_hook\" , \"param_human_name\" : \"Log received hook\" , \"param_description\" : \"Logs a message upon hook receiving if set to true. Otherwise do nothing.\" , \"default\" : True , \"mandatory\" : True , \"type\" : \"bool\" } ] The module configuration parameters are the following : param_name : The internal parameter name. This will be used by the module itself to fetch the value when needed. param_human_name : The name displayed on the UI for this specific parameter param_description : A description explaining what this parameter is doing to help administrators default : The default value of our parameter. Here we set to True, so after install our module is already configured and ready to log the hook. mandatory : Indicates whether the parameter is mandatory or not. If set to True and no value is provided (either by admin or by default), the module is automatically disabled by IRIS type : The type of parameter. Here a boolean, which will be rendered under the form of a checkbox. A module can have as many parameters as it needs. We now need to update our main class to set this configuration. iris_dummy_module/IrisDummyInterface.py #!/usr/bin/env python3 # Import the IrisInterface class from iris_interface.IrisModuleInterface import IrisModuleInterface # Create our module class class IrisDummyModule ( IrisModuleInterface ): # Set the configuration _module_name = interface_conf . module_name _module_description = interface_conf . module_description _interface_version = interface_conf . interface_version _module_version = interface_conf . module_version _pipeline_support = interface_conf . pipeline_support _pipeline_info = interface_conf . pipeline_info _module_configuration = interface_conf . module_configuration _module_type = interface_conf . module_type pass Done ! The module is now providing enough information to IRIS, so it knows exactly what our module do and what needs to be called to run it. However, our module is still doing nothing. Let's make it subscribe to an IRIS hook. Subscribing to a hook Hooks allow to be notified by IRIS when a specific event occurs (IOC creation, deletion, etc). For a comprehensive description of hooks, please see the Hooks section of this documentation. The registration (or subscription) to a hook occurs at two moments during the life of a module : When the module is added to IRIS When the configuration of the module is changed by an Admin. This allows dynamic subscription and deregistration of hooks depending on the config. These registration/deregistration events are triggered by IRIS, and are propagated to modules through the IrisModuleInterface method register_hooks [ ref ]. To register to a hook, we need to override this method and register our hook within this method. To do so, IrisModuleInterface offers us another method register_to_hook [ ref ], which we can call for each hook we want to subscribe. Here is a summary of the events : IRIS calls register_hooks of our module. This indicates it is time for us to register our hooks. Within this method, we call register_to_hook for each hook we want to subscribe Let's add this to our main class and register to the on_postload_ioc_create . This will notify use each time a new IOC is created and committed to the database. iris_dummy_module/IrisDummyInterface.py #!/usr/bin/env python3 # Import the IrisInterface class from iris_interface.IrisModuleInterface import IrisModuleInterface # Create our module class class IrisDummyModule ( IrisModuleInterface ): # Set the configuration _module_name = interface_conf . module_name _module_description = interface_conf . module_description _interface_version = interface_conf . interface_version _module_version = interface_conf . module_version _pipeline_support = interface_conf . pipeline_support _pipeline_info = interface_conf . pipeline_info _module_configuration = interface_conf . module_configuration _module_type = interface_conf . module_type def register_hooks ( self , module_id : int ): \"\"\" Called by IRIS indicating it's time to register hooks. :param module_id: Module ID provided by IRIS. \"\"\" # Call the hook registration method. We need to pass the # the module_id to this method, otherwise IRIS won't know # to whom associate the hook. # The hook name needs to be a well known hook name by IRIS. status = self . register_to_hook ( module_id , iris_hook_name = 'on_postload_ioc_create' ) if status . is_failure (): # If we have a failure, log something out self . log . error ( status . get_message ()) else : # Log that we successfully registered to the hook self . log . info ( f \"Successfully subscribed to on_postload_ioc_create hook\" ) That's it ! Our module has now officially subscribed to a hook and will be notified each time an IOC is created. So how the module is notified ? Once again this is done by a method named hooks_handler [ ref ] that IrisModuleInterface provides, and we need to overwrite. This method is called each time one of the event associated to the hook we subscribed is triggered. It provides the name of the hook and as well as the data associated to it. By overwriting this method, we can process the hook and the data ! We will add a condition in this method, that is if the administrator set the module parameter log_received_hook to False, then the module won't log anything and simply return the data. Hint The current configuration of the module can be accessed with the attribute self._dict_conf . iris_dummy_module/IrisDummyInterface.py #!/usr/bin/env python3 # Import the IrisInterface class from iris_interface.IrisModuleInterface import IrisModuleInterface # Create our module class class IrisDummyModule ( IrisModuleInterface ): # Set the configuration _module_name = interface_conf . module_name _module_description = interface_conf . module_description _interface_version = interface_conf . interface_version _module_version = interface_conf . module_version _pipeline_support = interface_conf . pipeline_support _pipeline_info = interface_conf . pipeline_info _module_configuration = interface_conf . module_configuration _module_type = interface_conf . module_type def register_hooks ( self , module_id : int ): \"\"\" Called by IRIS indicating it's time to register hooks. :param module_id: Module ID provided by IRIS. \"\"\" # Call the hook registration method. We need to pass the # the module_id to this method, otherwise IRIS won't know # to whom associate the hook. # The hook name needs to be a well known hook name by IRIS. status = self . register_to_hook ( module_id , iris_hook_name = 'on_postload_ioc_create' ) if status . is_failure (): # If we have a failure, log something out self . log . error ( status . get_message ()) else : # Log that we successfully registered to the hook self . log . info ( f \"Successfully subscribed to on_postload_ioc_create hook\" ) def hooks_handler ( self , hook_name : str , data ): \"\"\" Called by IRIS each time one of our hook is triggered. \"\"\" # read the current configuration and only log the call if # our parameter is set to true if self . _dict_conf . get ( 'log_received_hook' ) is True : self . log . info ( f 'Received { hook_name } ' ) self . log . info ( f 'Received data of type { type ( data ) } ' ) # Return a standardized message to IRIS saying that everything is ok. # logs=list(self.message_queue) is needed, so the users can see the logs # our module generated during its execution. return InterfaceStatus . I2Success ( data = data , logs = list ( self . message_queue )) We are done ! Our module is now fully ready to register, subscribe to a hook and act when notified.","title":"Processor modules"},{"location":"development/quick_start/processor/#processor-modules","text":"In this tutorial, we demonstrate the steps to write a basic processor module which subscribes to a hook, and log what it receives when the hook is triggered. We will also add a configuration setting to offer our users the ability disable this feature. We'll call it IrisDummyModule .","title":"Processor modules"},{"location":"development/quick_start/processor/#project-structure-overview","text":"As described in the development module main page , the module should have the following structure. setup . py # Setup configuration to build the module README . md # README iris_dummy_module # Name of the package __init__ . py # Declaration of the package and main class IrisDummyConfig . py # Configuration of the module to help keep the main file clean IrisDummyInterface . py # Main class of the module While the module could have only one main file IrisDummyInterface.py , we recommend splitting its configuration into a new configuration file (here IrisDummyConfig.py ) to keep the code clear. There is no mandatory naming convention for the files or the class or the methods. We chose this one to keep things clear, and we recommend following the same. But it's up to you really. We will walk over these files one by one during this tutorial.","title":"Project structure overview"},{"location":"development/quick_start/processor/#creating-the-interface","text":"The interface is the code that talks with IRIS. It implements methods that call and are called by the server. It needs to inherit IrisModuleInterface class from the IrisModuleInterface package . This module handles most of the methods needed by IRIS to recognize, set up and call the module. By inheriting this class in our interface, we avoid writing that part ourselves. Let's write our basic interface class. The name of the file has to be the name of the main class, that's the only constraint. We'll see later on why. iris_dummy_module/IrisDummyInterface.py #!/usr/bin/env python3 # Import the IrisInterface class from iris_interface.IrisModuleInterface import IrisModuleInterface # Create our module class class IrisDummyModule ( IrisModuleInterface ): pass That's it ! Actually this class is not doing anything right now. We'll need to add a few methods to register our hook later. But first we need to indicate to IRIS what is our main interface class. Remember, there is no convention restriction, so IRIS has no way to know which class it should instantiate to call our module. To do so, we need to set a specific variable in our __init__.py . iris_dummy_module/__init__.py # Set the __iris_module_interface variable to the name of our main class. # When IRIS instantiate the python module, it looks for \"module.__iris_module_interface\" # And then tries to instantiate the class \"__iris_module_interface.__iris_module_interface\", here 'IrisDummyModule.IrisDummyModule'. # That's why the python file must have the same name as the class. __iris_module_interface = \"IrisDummyModule\" Our module is now recognizable by IRIS Pretty simple right ?","title":"Creating the interface"},{"location":"development/quick_start/processor/#writing-the-configuration","text":"The next step is to describe what the module is doing, its name, it's configuration, etc. This is done by overwriting predefines variables of the IrisModuleInterface class. Let's create our Python configuration file and go through each variables. iris_dummy_module/IrisDummyConfig.py # Import the module types list, so we can indicate the type of our module from iris_interface.IrisModuleInterface import IrisModuleTypes # Human name displayed in the GUI Manage > Modules. This can be anything, # but try to put something meaningful, so users recognize your module. module_name = \"IrisDummy\" # Description displayed when editing the module configuration in the UI. # This can be anything, module_description = \"Provides a dummy module that replies to one hook\" # Set the interface version used. This needs to be the version of # the IrisModuleInterface package. This version is check by the server to # to ensure our module can run on this specific server interface_version = 1.1 # The version of the module itself, it can be anything module_version = 1.0 # The type of the module, here processor module_type = IrisModuleTypes . module_processor # Our module is a processor type, so it doesn't offer any pipeline pipeline_support = False # Provide no pipeline information as our module don't implement any pipeline_info = {} # The configuration of the module that will be displayed and configurable # by administrators on the UI. This describes every parameter that can # be set. module_configuration = [ { \"param_name\" : \"log_received_hook\" , \"param_human_name\" : \"Log received hook\" , \"param_description\" : \"Logs a message upon hook receiving if set to true. Otherwise do nothing.\" , \"default\" : True , \"mandatory\" : True , \"type\" : \"bool\" } ] The module configuration parameters are the following : param_name : The internal parameter name. This will be used by the module itself to fetch the value when needed. param_human_name : The name displayed on the UI for this specific parameter param_description : A description explaining what this parameter is doing to help administrators default : The default value of our parameter. Here we set to True, so after install our module is already configured and ready to log the hook. mandatory : Indicates whether the parameter is mandatory or not. If set to True and no value is provided (either by admin or by default), the module is automatically disabled by IRIS type : The type of parameter. Here a boolean, which will be rendered under the form of a checkbox. A module can have as many parameters as it needs. We now need to update our main class to set this configuration. iris_dummy_module/IrisDummyInterface.py #!/usr/bin/env python3 # Import the IrisInterface class from iris_interface.IrisModuleInterface import IrisModuleInterface # Create our module class class IrisDummyModule ( IrisModuleInterface ): # Set the configuration _module_name = interface_conf . module_name _module_description = interface_conf . module_description _interface_version = interface_conf . interface_version _module_version = interface_conf . module_version _pipeline_support = interface_conf . pipeline_support _pipeline_info = interface_conf . pipeline_info _module_configuration = interface_conf . module_configuration _module_type = interface_conf . module_type pass Done ! The module is now providing enough information to IRIS, so it knows exactly what our module do and what needs to be called to run it. However, our module is still doing nothing. Let's make it subscribe to an IRIS hook.","title":"Writing the configuration"},{"location":"development/quick_start/processor/#subscribing-to-a-hook","text":"Hooks allow to be notified by IRIS when a specific event occurs (IOC creation, deletion, etc). For a comprehensive description of hooks, please see the Hooks section of this documentation. The registration (or subscription) to a hook occurs at two moments during the life of a module : When the module is added to IRIS When the configuration of the module is changed by an Admin. This allows dynamic subscription and deregistration of hooks depending on the config. These registration/deregistration events are triggered by IRIS, and are propagated to modules through the IrisModuleInterface method register_hooks [ ref ]. To register to a hook, we need to override this method and register our hook within this method. To do so, IrisModuleInterface offers us another method register_to_hook [ ref ], which we can call for each hook we want to subscribe. Here is a summary of the events : IRIS calls register_hooks of our module. This indicates it is time for us to register our hooks. Within this method, we call register_to_hook for each hook we want to subscribe Let's add this to our main class and register to the on_postload_ioc_create . This will notify use each time a new IOC is created and committed to the database. iris_dummy_module/IrisDummyInterface.py #!/usr/bin/env python3 # Import the IrisInterface class from iris_interface.IrisModuleInterface import IrisModuleInterface # Create our module class class IrisDummyModule ( IrisModuleInterface ): # Set the configuration _module_name = interface_conf . module_name _module_description = interface_conf . module_description _interface_version = interface_conf . interface_version _module_version = interface_conf . module_version _pipeline_support = interface_conf . pipeline_support _pipeline_info = interface_conf . pipeline_info _module_configuration = interface_conf . module_configuration _module_type = interface_conf . module_type def register_hooks ( self , module_id : int ): \"\"\" Called by IRIS indicating it's time to register hooks. :param module_id: Module ID provided by IRIS. \"\"\" # Call the hook registration method. We need to pass the # the module_id to this method, otherwise IRIS won't know # to whom associate the hook. # The hook name needs to be a well known hook name by IRIS. status = self . register_to_hook ( module_id , iris_hook_name = 'on_postload_ioc_create' ) if status . is_failure (): # If we have a failure, log something out self . log . error ( status . get_message ()) else : # Log that we successfully registered to the hook self . log . info ( f \"Successfully subscribed to on_postload_ioc_create hook\" ) That's it ! Our module has now officially subscribed to a hook and will be notified each time an IOC is created. So how the module is notified ? Once again this is done by a method named hooks_handler [ ref ] that IrisModuleInterface provides, and we need to overwrite. This method is called each time one of the event associated to the hook we subscribed is triggered. It provides the name of the hook and as well as the data associated to it. By overwriting this method, we can process the hook and the data ! We will add a condition in this method, that is if the administrator set the module parameter log_received_hook to False, then the module won't log anything and simply return the data. Hint The current configuration of the module can be accessed with the attribute self._dict_conf . iris_dummy_module/IrisDummyInterface.py #!/usr/bin/env python3 # Import the IrisInterface class from iris_interface.IrisModuleInterface import IrisModuleInterface # Create our module class class IrisDummyModule ( IrisModuleInterface ): # Set the configuration _module_name = interface_conf . module_name _module_description = interface_conf . module_description _interface_version = interface_conf . interface_version _module_version = interface_conf . module_version _pipeline_support = interface_conf . pipeline_support _pipeline_info = interface_conf . pipeline_info _module_configuration = interface_conf . module_configuration _module_type = interface_conf . module_type def register_hooks ( self , module_id : int ): \"\"\" Called by IRIS indicating it's time to register hooks. :param module_id: Module ID provided by IRIS. \"\"\" # Call the hook registration method. We need to pass the # the module_id to this method, otherwise IRIS won't know # to whom associate the hook. # The hook name needs to be a well known hook name by IRIS. status = self . register_to_hook ( module_id , iris_hook_name = 'on_postload_ioc_create' ) if status . is_failure (): # If we have a failure, log something out self . log . error ( status . get_message ()) else : # Log that we successfully registered to the hook self . log . info ( f \"Successfully subscribed to on_postload_ioc_create hook\" ) def hooks_handler ( self , hook_name : str , data ): \"\"\" Called by IRIS each time one of our hook is triggered. \"\"\" # read the current configuration and only log the call if # our parameter is set to true if self . _dict_conf . get ( 'log_received_hook' ) is True : self . log . info ( f 'Received { hook_name } ' ) self . log . info ( f 'Received data of type { type ( data ) } ' ) # Return a standardized message to IRIS saying that everything is ok. # logs=list(self.message_queue) is needed, so the users can see the logs # our module generated during its execution. return InterfaceStatus . I2Success ( data = data , logs = list ( self . message_queue )) We are done ! Our module is now fully ready to register, subscribe to a hook and act when notified.","title":"Subscribing to a hook"},{"location":"operations/api/","text":"API Iris is meant to be plug-able to fit and integrates with the existing environments. Through the REST API, one can do almost as much as it is possible to do through the web interface. API Keys The first step is to obtain an API key. Each user is automatically attributed an API token. It can be found on the left panel, under username and My Settings. Token exposure In case the token is exposed and needs to be change, a new one can be generated with the Renew option. Renewing a token revokes the previous. References API v1.0.0 (applies to Iris <= v1.2.1) API v1.0.1 (applies to Iris > v1.2.1) API v1.0.2 (applies to Iris > v1.4.0) How to use The API token is a Bearer and needs to be present in the header Authorization when issuing requests. For example, to list all the cases: curl --request GET \\ --url http://localhost:8000/manage/cases/list \\ --header 'Authorization: Bearer mWpCUVNzBMU5EnbIAK50jLPhYjKBTHZjobdogc_n_yixpJTmt9tzAf8WYDI7m5XgB9wCJnlaXlHIh9RZjtp2fA' \\ --header 'Content-Type: application/json' The only way to revoke a token is to renew the current one. Once done, the previous API token does not exist anymore on the database and it becomes ineffective. A Python client is available here to ease the automation.","title":"API"},{"location":"operations/api/#api","text":"Iris is meant to be plug-able to fit and integrates with the existing environments. Through the REST API, one can do almost as much as it is possible to do through the web interface.","title":"API"},{"location":"operations/api/#api-keys","text":"The first step is to obtain an API key. Each user is automatically attributed an API token. It can be found on the left panel, under username and My Settings. Token exposure In case the token is exposed and needs to be change, a new one can be generated with the Renew option. Renewing a token revokes the previous.","title":"API Keys"},{"location":"operations/api/#references","text":"API v1.0.0 (applies to Iris <= v1.2.1) API v1.0.1 (applies to Iris > v1.2.1) API v1.0.2 (applies to Iris > v1.4.0)","title":"References"},{"location":"operations/api/#how-to-use","text":"The API token is a Bearer and needs to be present in the header Authorization when issuing requests. For example, to list all the cases: curl --request GET \\ --url http://localhost:8000/manage/cases/list \\ --header 'Authorization: Bearer mWpCUVNzBMU5EnbIAK50jLPhYjKBTHZjobdogc_n_yixpJTmt9tzAf8WYDI7m5XgB9wCJnlaXlHIh9RZjtp2fA' \\ --header 'Content-Type: application/json' The only way to revoke a token is to renew the current one. Once done, the previous API token does not exist anymore on the database and it becomes ineffective. A Python client is available here to ease the automation.","title":"How to use"},{"location":"operations/custom_attributes/","text":"Custom Attributes Introduced in IRIS v1.4.0 All the case objects can be extended with custom attributes. These attributes can be added by : Administrators via the GUI Modules (for instance, the VT module adds a VT Report attribute to each objects it analyses) Attributes offer the ability to : Add inputs for analysts to fill additional details Add static/dynamic content such as HTML/JS for enhanced possibilities. This section only describes how an administrator can add or delete attributes to an object. Management page Custom attributes can be changed in the Advanced > Objects Attributes section on the left panel. The page lists the objects for which custom attributes can be added or modified. Cases Customers Evidences Notes Tasks Assets Events IOC Attributes structure Attributes are defined in JSON which describes tabs and fields that makes the attributes. { \"Tab Name 1\" : { // De f i nes a ne w ta b \"Field 1\" : { // De f i nes a ne w f ield wi t hi n t he Tab Name 1 \"type\" : \"input_string\" , // De f i nes t he t ype o f f ield , here a s tan dard s tr i n g i n pu t \"mandatory\" : true , // I n dica tes whe t her t he f ield is ma n da t ory upo n savi n g \"value\" : \"\" // De fault value i f a n y , else emp t y }, \"Field 2\" : { // De f i nes a seco n d f ield wi t hi n t he ta b Tab Name 1 \"type\" : \"input_checkbox\" , // De f i nes a n i n pu t checkbox \"mandatory\" : false , // I n dica tes whe t her t he f ield is ma n da t ory upo n savi n g \"value\" : \"\" // De fault value i f a n y , else emp t y } }, \"VT report\" : { // De f i nes a seco n d ta b na med VT repor t \"Content\" : { // De f i nes a ne w f ield Co ntent wi t hi n t he VT Repor t \"type\" : \"html\" , // De f i nes a n HTML i nter pre te d co ntent \"value\" : \"\" // De fault value i f a n y , else emp t y } } } The code above would be rendered as : With : The native information of the object. This cannot be changed or updated The new attribute Tab Name 1 The other new attribute VT report Attributes taxonomy The available fields type are available for rendering : input_string : Standard input text input_textfield : Standard input textfield input_checkbox : Standard checkbox input_date : Standard date input input_datetime : Standard date and time input input_select : Standard select input. Need \"options\" tag to describe the available options, as a list of string. raw : A static content rendered in raw text. HTML is not be interpreted. html : A static content rendered as HTML. This is by nature prone to abuse, but at the same time allows adding custom JS scripts. Updating / resetting attributes When an attribute is updated, it will try to update all the existing objects with the new attributes. To prevent any data loss from previous attributes and attributes pushed by modules, the update is only made on attributes which don't have any values set or are type-compatibles (ie string to textfield). The migration of an attribute can however be forced in two ways, both resulting in potential attributes data loss. Good to know Migrating or overwriting attributes never change the native information of an object. It only applies to custom attributes. Partial overwrite basically resets all the values of every target objects that matches the current attribute definition. All associated values are lost. This does not impact attributes pushed by modules or previous configuration. Complete overwrite resets all attributes of every target objects, including the ones created by modules, and then applies the current attributes. All associated values are lost.","title":"Custom Attributes"},{"location":"operations/custom_attributes/#custom-attributes","text":"Introduced in IRIS v1.4.0 All the case objects can be extended with custom attributes. These attributes can be added by : Administrators via the GUI Modules (for instance, the VT module adds a VT Report attribute to each objects it analyses) Attributes offer the ability to : Add inputs for analysts to fill additional details Add static/dynamic content such as HTML/JS for enhanced possibilities. This section only describes how an administrator can add or delete attributes to an object.","title":"Custom Attributes"},{"location":"operations/custom_attributes/#management-page","text":"Custom attributes can be changed in the Advanced > Objects Attributes section on the left panel. The page lists the objects for which custom attributes can be added or modified. Cases Customers Evidences Notes Tasks Assets Events IOC","title":"Management page"},{"location":"operations/custom_attributes/#attributes-structure","text":"Attributes are defined in JSON which describes tabs and fields that makes the attributes. { \"Tab Name 1\" : { // De f i nes a ne w ta b \"Field 1\" : { // De f i nes a ne w f ield wi t hi n t he Tab Name 1 \"type\" : \"input_string\" , // De f i nes t he t ype o f f ield , here a s tan dard s tr i n g i n pu t \"mandatory\" : true , // I n dica tes whe t her t he f ield is ma n da t ory upo n savi n g \"value\" : \"\" // De fault value i f a n y , else emp t y }, \"Field 2\" : { // De f i nes a seco n d f ield wi t hi n t he ta b Tab Name 1 \"type\" : \"input_checkbox\" , // De f i nes a n i n pu t checkbox \"mandatory\" : false , // I n dica tes whe t her t he f ield is ma n da t ory upo n savi n g \"value\" : \"\" // De fault value i f a n y , else emp t y } }, \"VT report\" : { // De f i nes a seco n d ta b na med VT repor t \"Content\" : { // De f i nes a ne w f ield Co ntent wi t hi n t he VT Repor t \"type\" : \"html\" , // De f i nes a n HTML i nter pre te d co ntent \"value\" : \"\" // De fault value i f a n y , else emp t y } } } The code above would be rendered as : With : The native information of the object. This cannot be changed or updated The new attribute Tab Name 1 The other new attribute VT report","title":"Attributes structure"},{"location":"operations/custom_attributes/#attributes-taxonomy","text":"The available fields type are available for rendering : input_string : Standard input text input_textfield : Standard input textfield input_checkbox : Standard checkbox input_date : Standard date input input_datetime : Standard date and time input input_select : Standard select input. Need \"options\" tag to describe the available options, as a list of string. raw : A static content rendered in raw text. HTML is not be interpreted. html : A static content rendered as HTML. This is by nature prone to abuse, but at the same time allows adding custom JS scripts.","title":"Attributes taxonomy"},{"location":"operations/custom_attributes/#updating-resetting-attributes","text":"When an attribute is updated, it will try to update all the existing objects with the new attributes. To prevent any data loss from previous attributes and attributes pushed by modules, the update is only made on attributes which don't have any values set or are type-compatibles (ie string to textfield). The migration of an attribute can however be forced in two ways, both resulting in potential attributes data loss. Good to know Migrating or overwriting attributes never change the native information of an object. It only applies to custom attributes. Partial overwrite basically resets all the values of every target objects that matches the current attribute definition. All associated values are lost. This does not impact attributes pushed by modules or previous configuration. Complete overwrite resets all attributes of every target objects, including the ones created by modules, and then applies the current attributes. All associated values are lost.","title":"Updating / resetting attributes"},{"location":"operations/modules/","text":"Modules IRIS can be extended with modules. They can be split in two types: Pipeline modules : Allow upload and process of evidences through modular pipelines (eg: EVTX parsing and injection into a database or data visualiser) Processor modules : Allow processing of IRIS data upon predefined actions / hooks. (eg: be notified when a new IOC is created and get VT/MISP insights for it). Modules (or DIM - DFIR-IRIS Modules) are actually Python packages which must be installed in the Python environment of iris-webapp and the worker (see Quick Start). Once installed in the Python environment, modules can be managed in Advanced > Modules . Info This section is only available for users with the Admin role. By default IRIS embeds multiple DIMs. IrisEVTXModule (installed but not registered) IrisVTModule (installed and registered) IrisMispModule (installed and registered) Let's take IrisEVTXModule one to explain how one can add and use a new module. Module management To add a module, the user can click on the \"+\" button: Then the user must enter the name of the pre-installed module. The name of the pip package must be used. If everything is ok, the module will appear on the list. It is currently disabled, and needs configuration before it can be enabled. To do so, the user can click on the module's name: A new text box appears, showing information about the module, and a list of parameters to configure. Each mandatory parameter must be configured to enable the module. After configuring all the mandatory parameters, the \"Enable button\" is revealed and the user can finally enable the module. That's all! The user can confirm in the summary that the module is indeed enabled and ready to use. Finally, the user can either disable or remove the module by clicking on the according buttons. Now that the module is configured and enabled, let's see how we can use it! NB: As a temporary fix, after adding and configuring a module, one must restart the IRIS services (dockers) else the worker won't have the module installed properly. How to use the module As stated in the beginning, a module extends the capabilities of IRIS. For now, it allows importing evidences of your needs into what we call a pipeline, where data will be handled in the module (checking, parsing, ingestion...). In our provided module, IrisEVTXModule ingest EVTX files, parse them as JSON, and send the results to a Splunk instance using its HTTP event collector (HEC) endpoint. In IRIS, the files are always imported in the context of a case. To import a file, the user can click on Manage cases then Update tab. In Processing pipeline , the user can pick a pipeline that will send the files to the wanted module. In our example, EVTX pipeline refers to the IrisEVTXModule module. Below, the user can fill the arguments needed by the according pipeline. Arguments can be optional. Finally, the user can import one or several files and click Update to start their processing by the module. You can see in the picture below that the user will import four EVTX files. The user can follow the upload of the different files with their respective progress bars. Once uploaded, the status of the task can be observed on the DIM Tasks page. Clicking on a Task ID shows information on the task processing. After the processing of the files by the module, the list of the imported files is stored in the Evidences tab of the according case.","title":"Modules"},{"location":"operations/modules/#modules","text":"IRIS can be extended with modules. They can be split in two types: Pipeline modules : Allow upload and process of evidences through modular pipelines (eg: EVTX parsing and injection into a database or data visualiser) Processor modules : Allow processing of IRIS data upon predefined actions / hooks. (eg: be notified when a new IOC is created and get VT/MISP insights for it). Modules (or DIM - DFIR-IRIS Modules) are actually Python packages which must be installed in the Python environment of iris-webapp and the worker (see Quick Start). Once installed in the Python environment, modules can be managed in Advanced > Modules . Info This section is only available for users with the Admin role. By default IRIS embeds multiple DIMs. IrisEVTXModule (installed but not registered) IrisVTModule (installed and registered) IrisMispModule (installed and registered) Let's take IrisEVTXModule one to explain how one can add and use a new module.","title":"Modules"},{"location":"operations/modules/#module-management","text":"To add a module, the user can click on the \"+\" button: Then the user must enter the name of the pre-installed module. The name of the pip package must be used. If everything is ok, the module will appear on the list. It is currently disabled, and needs configuration before it can be enabled. To do so, the user can click on the module's name: A new text box appears, showing information about the module, and a list of parameters to configure. Each mandatory parameter must be configured to enable the module. After configuring all the mandatory parameters, the \"Enable button\" is revealed and the user can finally enable the module. That's all! The user can confirm in the summary that the module is indeed enabled and ready to use. Finally, the user can either disable or remove the module by clicking on the according buttons. Now that the module is configured and enabled, let's see how we can use it! NB: As a temporary fix, after adding and configuring a module, one must restart the IRIS services (dockers) else the worker won't have the module installed properly.","title":"Module management"},{"location":"operations/modules/#how-to-use-the-module","text":"As stated in the beginning, a module extends the capabilities of IRIS. For now, it allows importing evidences of your needs into what we call a pipeline, where data will be handled in the module (checking, parsing, ingestion...). In our provided module, IrisEVTXModule ingest EVTX files, parse them as JSON, and send the results to a Splunk instance using its HTTP event collector (HEC) endpoint. In IRIS, the files are always imported in the context of a case. To import a file, the user can click on Manage cases then Update tab. In Processing pipeline , the user can pick a pipeline that will send the files to the wanted module. In our example, EVTX pipeline refers to the IrisEVTXModule module. Below, the user can fill the arguments needed by the according pipeline. Arguments can be optional. Finally, the user can import one or several files and click Update to start their processing by the module. You can see in the picture below that the user will import four EVTX files. The user can follow the upload of the different files with their respective progress bars. Once uploaded, the status of the task can be observed on the DIM Tasks page. Clicking on a Task ID shows information on the task processing. After the processing of the files by the module, the list of the imported files is stored in the Evidences tab of the according case.","title":"How to use the module"},{"location":"operations/reports/","text":"Reports Iris has the ability to generate reports based on the data of an investigation. The reports templates can be managed in Advanced > Templates . Info This section is only available for users with the Admin role. Structure of templates Reports templates are made of tags, which are then processed and filed by the template engine of Iris. The templates can have any forms as soon as they respect the tags. An example of report is available in the source code of Iris, source > app > templates > docx_reports . The following tags are available. None are mandatory. If a tag is mistyped, the generation step will produce an error message. Hint Standard objects are accessible with {{ objectname }} . List objects can be looped: {% for object in object_list %} {{ object.attribute }} {% endfor %} case.name : Name of the case case.description : Description of the case case.open_date : Case open date case.close_date : Case close date case.opened_by : User that initially opened the case case.for_customer : Customer linked to the case case.soc_id : SOC ID number linked to the case evidences : List of evidence objects (see below - given evidence as loop variable) evidence.filename : File name of the evidence evidence.date_added : Date of registration evidence.file_hash : Hash of the evidence evidence.added_by : User who added the evidence iocs : List of IOCs objects (see below - given ioc as loop variable) ioc.ioc_value : Value of the IOC ioc.ioc_description : Description of the IOC ioc.ioc_type : Type of IOC ioc.ioc_tags : Tags linked to the IOC notes : List of notes objects (see below - given note as loop variable) note.note_title : Title of the note note.note_content : Content of the note note.note_creationdate : Creation date of the note note.note_lastupdate : Date of last update tasks : List of tasks objects (see below - given task as loop variable) task.task_title : Title of the task task.task_description : Description of the task task.task_open_date : Open date of the task task.task_last_update : Last update of the task task.task_close_date : Date of closure task.task_status : Status of the task task.task_tags : Task for the tags timeline : List of events objects (see below - given event as loop variable) event.event_title : Title of the event event.event_content : Content of the event event.event_raw : Raw content of the event event.event_date : Date when the event happened event.event_source : Source of the event event.category : Category of the event event.event_tags : Tags of the events event.last_edited_by : User who last edited the event event.assets : List of assets names linked to the event","title":"Reports"},{"location":"operations/reports/#reports","text":"Iris has the ability to generate reports based on the data of an investigation. The reports templates can be managed in Advanced > Templates . Info This section is only available for users with the Admin role.","title":"Reports"},{"location":"operations/reports/#structure-of-templates","text":"Reports templates are made of tags, which are then processed and filed by the template engine of Iris. The templates can have any forms as soon as they respect the tags. An example of report is available in the source code of Iris, source > app > templates > docx_reports . The following tags are available. None are mandatory. If a tag is mistyped, the generation step will produce an error message. Hint Standard objects are accessible with {{ objectname }} . List objects can be looped: {% for object in object_list %} {{ object.attribute }} {% endfor %} case.name : Name of the case case.description : Description of the case case.open_date : Case open date case.close_date : Case close date case.opened_by : User that initially opened the case case.for_customer : Customer linked to the case case.soc_id : SOC ID number linked to the case evidences : List of evidence objects (see below - given evidence as loop variable) evidence.filename : File name of the evidence evidence.date_added : Date of registration evidence.file_hash : Hash of the evidence evidence.added_by : User who added the evidence iocs : List of IOCs objects (see below - given ioc as loop variable) ioc.ioc_value : Value of the IOC ioc.ioc_description : Description of the IOC ioc.ioc_type : Type of IOC ioc.ioc_tags : Tags linked to the IOC notes : List of notes objects (see below - given note as loop variable) note.note_title : Title of the note note.note_content : Content of the note note.note_creationdate : Creation date of the note note.note_lastupdate : Date of last update tasks : List of tasks objects (see below - given task as loop variable) task.task_title : Title of the task task.task_description : Description of the task task.task_open_date : Open date of the task task.task_last_update : Last update of the task task.task_close_date : Date of closure task.task_status : Status of the task task.task_tags : Task for the tags timeline : List of events objects (see below - given event as loop variable) event.event_title : Title of the event event.event_content : Content of the event event.event_raw : Raw content of the event event.event_date : Date when the event happened event.event_source : Source of the event event.category : Category of the event event.event_tags : Tags of the events event.last_edited_by : User who last edited the event event.assets : List of assets names linked to the event","title":"Structure of templates"},{"location":"operations/tutorials/","text":"Tutorials Good to know These tutorials are based on IRIS v1.2.1 and offers only a basic overview of IRIS. New versions offer more features, not showcased on this page. Cases Create a new case Your browser does not support the video tag. Case summary Your browser does not support the video tag. Case notes Your browser does not support the video tag. Case IOCs Your browser does not support the video tag. Case assets Your browser does not support the video tag. Case timeline Your browser does not support the video tag. Case graph Your browser does not support the video tag. Case tasks Your browser does not support the video tag. Case evidences Your browser does not support the video tag. Case task logs Your browser does not support the video tag. Case reports Your browser does not support the video tag. Operations Global searches Your browser does not support the video tag.","title":"Tutorials"},{"location":"operations/tutorials/#tutorials","text":"Good to know These tutorials are based on IRIS v1.2.1 and offers only a basic overview of IRIS. New versions offer more features, not showcased on this page.","title":"Tutorials"},{"location":"operations/tutorials/#cases","text":"","title":"Cases"},{"location":"operations/tutorials/#create-a-new-case","text":"Your browser does not support the video tag.","title":"Create a new case"},{"location":"operations/tutorials/#case-summary","text":"Your browser does not support the video tag.","title":"Case summary"},{"location":"operations/tutorials/#case-notes","text":"Your browser does not support the video tag.","title":"Case notes"},{"location":"operations/tutorials/#case-iocs","text":"Your browser does not support the video tag.","title":"Case IOCs"},{"location":"operations/tutorials/#case-assets","text":"Your browser does not support the video tag.","title":"Case assets"},{"location":"operations/tutorials/#case-timeline","text":"Your browser does not support the video tag.","title":"Case timeline"},{"location":"operations/tutorials/#case-graph","text":"Your browser does not support the video tag.","title":"Case graph"},{"location":"operations/tutorials/#case-tasks","text":"Your browser does not support the video tag.","title":"Case tasks"},{"location":"operations/tutorials/#case-evidences","text":"Your browser does not support the video tag.","title":"Case evidences"},{"location":"operations/tutorials/#case-task-logs","text":"Your browser does not support the video tag.","title":"Case task logs"},{"location":"operations/tutorials/#case-reports","text":"Your browser does not support the video tag.","title":"Case reports"},{"location":"operations/tutorials/#operations","text":"","title":"Operations"},{"location":"operations/tutorials/#global-searches","text":"Your browser does not support the video tag.","title":"Global searches"},{"location":"operations/upgrades/","text":"Upgrades Most of the time, Iris handles upgrades of the database automatically when a new version is started, thus no specific actions are required. However , some breaking changes might need manual intervention. Please use the table below to assess if a manual action is required. From / To v1.2.1 v1.3.0 v1.3.1 v1.4.0 v1.2.1 X Auto Auto Action required - See v1.4.0 v1.3.0 X X Auto Action required - See v1.4.0 v1.3.1 X X X Action required - See v1.4.0 v1.4.0 X X X X Caution For production environments, it is highly recommended to make backups of the DB in case any issues occur during upgrades. Backing-up DB Only if you run in production and/or data is critical. List the current running docker containers docker container list Spot the IRIS DB container name or ID, and execute the backup docker exec <container> pg_dump -U postgres iris_db | \\ gzip > ../iris_db_backup.gz Ensure the backup was successful by looking at the gz file zcat ../iris_db_backup.gz | less Upgrading Stop the dockers docker-compose stop Remove the application dockers docker-compose rm app worker Get the last version of Iris git checkout master - or any tagged versions Build the new versions docker-compose build app worker --no-cache Run IRIS again. The app will handle the DB migration docker-compose up Rolling back In case something went wrong, you can rollback to your previous version and restore data. Remove the faulty docker DB docker-compose down db --volumes Checkout to the previous version working of IRIS Rebuild the images docker-compose build --no-cache Spin up the docker DB, and ONLY this one. docker-compose up db Get the ID or name of the docker DB docker container list Restore the DB data zcat ../iris_db_backup.gz | docker exec -i <container> psql -U postgres -d iris_db Spin up the rest of the dockers docker-compose up Your data should back. Version specific upgrades v1.4.0 This version brings breaking changes in the DB docker by adding a named volume instead of the default one. This implies that previous existing database is ignored as the new docker won't know which volume was previously used. To prevent this, please strictly follow the guide below . This will copy the data of the existing volume, to the new named one. Spot the IRIS DB container with docker container list . It should look like iris-web-db-x Fetch the current db volume ID ( name field with the command below) docker inspect <iris_db> | grep -A5 \"Mounts\" # Example of output \"Mounts\" : [ { \"Type\" : \"volume\" , \"Name\" : \"a90b9998a3233a68438c8e099bd0ba98d9f62c9734e40297b8067f9fdb921eb9\" , \"Source\" : \"/var/lib/docker/volumes/a90b9998a3233a68438c8e099bd0ba98d9f62c9734e40297b8067f9fdb921eb9/_data\" , \"Destination\" : \"/var/lib/postgresql/data\" , 3. Stop all the IRIS dockers : docker-compose stop 4. Create a new empty volume : docker volume create --name iris-web_db_data 5. Run a volume copy via a dummy image : docker run --rm -it -v <previous_db_volume_id>:/from:ro -v iris-web_db_data:/to alpine ash -c \"cd /from ; cp -av . /to\" # With the example of 2., this gives docker run --rm -it -v a90b9998a3233a68438c8e099bd0ba98d9f62c9734e40297b8067f9fdb921eb9:/from:ro -v iris-web_db_data:/to alpine ash -c \"cd /from ; cp -av . /to\" 6. Pull the last changes from the repository, checkout to v1.4.0 , build and run. git pull origin git checkout v1.4.0 docker-compose build docker-compose up 7. The data should be successfully transferred.","title":"Upgrades"},{"location":"operations/upgrades/#upgrades","text":"Most of the time, Iris handles upgrades of the database automatically when a new version is started, thus no specific actions are required. However , some breaking changes might need manual intervention. Please use the table below to assess if a manual action is required. From / To v1.2.1 v1.3.0 v1.3.1 v1.4.0 v1.2.1 X Auto Auto Action required - See v1.4.0 v1.3.0 X X Auto Action required - See v1.4.0 v1.3.1 X X X Action required - See v1.4.0 v1.4.0 X X X X Caution For production environments, it is highly recommended to make backups of the DB in case any issues occur during upgrades.","title":"Upgrades"},{"location":"operations/upgrades/#backing-up-db","text":"Only if you run in production and/or data is critical. List the current running docker containers docker container list Spot the IRIS DB container name or ID, and execute the backup docker exec <container> pg_dump -U postgres iris_db | \\ gzip > ../iris_db_backup.gz Ensure the backup was successful by looking at the gz file zcat ../iris_db_backup.gz | less","title":"Backing-up DB"},{"location":"operations/upgrades/#upgrading","text":"Stop the dockers docker-compose stop Remove the application dockers docker-compose rm app worker Get the last version of Iris git checkout master - or any tagged versions Build the new versions docker-compose build app worker --no-cache Run IRIS again. The app will handle the DB migration docker-compose up","title":"Upgrading"},{"location":"operations/upgrades/#rolling-back","text":"In case something went wrong, you can rollback to your previous version and restore data. Remove the faulty docker DB docker-compose down db --volumes Checkout to the previous version working of IRIS Rebuild the images docker-compose build --no-cache Spin up the docker DB, and ONLY this one. docker-compose up db Get the ID or name of the docker DB docker container list Restore the DB data zcat ../iris_db_backup.gz | docker exec -i <container> psql -U postgres -d iris_db Spin up the rest of the dockers docker-compose up Your data should back.","title":"Rolling back"},{"location":"operations/upgrades/#version-specific-upgrades","text":"","title":"Version specific upgrades"},{"location":"operations/upgrades/#v140","text":"This version brings breaking changes in the DB docker by adding a named volume instead of the default one. This implies that previous existing database is ignored as the new docker won't know which volume was previously used. To prevent this, please strictly follow the guide below . This will copy the data of the existing volume, to the new named one. Spot the IRIS DB container with docker container list . It should look like iris-web-db-x Fetch the current db volume ID ( name field with the command below) docker inspect <iris_db> | grep -A5 \"Mounts\" # Example of output \"Mounts\" : [ { \"Type\" : \"volume\" , \"Name\" : \"a90b9998a3233a68438c8e099bd0ba98d9f62c9734e40297b8067f9fdb921eb9\" , \"Source\" : \"/var/lib/docker/volumes/a90b9998a3233a68438c8e099bd0ba98d9f62c9734e40297b8067f9fdb921eb9/_data\" , \"Destination\" : \"/var/lib/postgresql/data\" , 3. Stop all the IRIS dockers : docker-compose stop 4. Create a new empty volume : docker volume create --name iris-web_db_data 5. Run a volume copy via a dummy image : docker run --rm -it -v <previous_db_volume_id>:/from:ro -v iris-web_db_data:/to alpine ash -c \"cd /from ; cp -av . /to\" # With the example of 2., this gives docker run --rm -it -v a90b9998a3233a68438c8e099bd0ba98d9f62c9734e40297b8067f9fdb921eb9:/from:ro -v iris-web_db_data:/to alpine ash -c \"cd /from ; cp -av . /to\" 6. Pull the last changes from the repository, checkout to v1.4.0 , build and run. git pull origin git checkout v1.4.0 docker-compose build docker-compose up 7. The data should be successfully transferred.","title":"v1.4.0"},{"location":"python_client/dfir_iris_client/","text":"Table of Contents","title":"Table of Contents"},{"location":"python_client/dfir_iris_client/#table-of-contents","text":"","title":"Table of Contents"},{"location":"python_client/modules/","text":"Python client dfir_iris_client offers a Python interface to communicate with IRIS. It relies exclusively on the API, which means output of the methods are the same as specified in the API reference. The source code of the project is available here . Versions The Python client version follows the API versions (until the patch level). Meaning for API v1.0.1, one need to install dfir_iris_client-1.0.1 . Examples Examples of usage are available here . References Doc v1.0.1","title":"Python client"},{"location":"python_client/modules/#python-client","text":"dfir_iris_client offers a Python interface to communicate with IRIS. It relies exclusively on the API, which means output of the methods are the same as specified in the API reference. The source code of the project is available here .","title":"Python client"},{"location":"python_client/modules/#versions","text":"The Python client version follows the API versions (until the patch level). Meaning for API v1.0.1, one need to install dfir_iris_client-1.0.1 .","title":"Versions"},{"location":"python_client/modules/#examples","text":"Examples of usage are available here .","title":"Examples"},{"location":"python_client/modules/#references","text":"Doc v1.0.1","title":"References"}]}